{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ir3cGqltcxLu",
        "outputId": "0c10be83-aac5-4d8a-e2f3-23773dc70a8f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting ucimlrepo\n",
            "  Downloading ucimlrepo-0.0.3-py3-none-any.whl (7.0 kB)\n",
            "Installing collected packages: ucimlrepo\n",
            "Successfully installed ucimlrepo-0.0.3\n"
          ]
        }
      ],
      "source": [
        "#https://archive.ics.uci.edu/dataset/29/computer+hardware\n",
        "%pip install ucimlrepo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "K0S3QvMec4hv",
        "outputId": "2d165d88-7c09-42df-a62a-750a0db8b736"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0.0833, 1.0000],\n",
            "        [0.0193, 0.1250],\n",
            "        [0.0193, 0.1250],\n",
            "        [0.0193, 0.1250],\n",
            "        [0.0193, 0.1250],\n",
            "        [0.0173, 0.2500],\n",
            "        [0.0153, 0.2500],\n",
            "        [0.0153, 0.2500],\n",
            "        [0.0153, 0.2500],\n",
            "        [0.0153, 0.5000],\n",
            "        [0.2667, 0.0000],\n",
            "        [0.2667, 0.0156],\n",
            "        [0.0400, 0.2539],\n",
            "        [0.0333, 0.2539],\n",
            "        [0.2333, 0.0000],\n",
            "        [0.1333, 0.0000],\n",
            "        [0.1113, 0.0312],\n",
            "        [0.0953, 0.0000],\n",
            "        [0.0953, 0.0000],\n",
            "        [0.0733, 0.5547],\n",
            "        [0.0953, 0.0000],\n",
            "        [0.0953, 0.0000],\n",
            "        [0.0953, 0.0000],\n",
            "        [0.0733, 0.0000],\n",
            "        [0.2133, 0.0000],\n",
            "        [0.2133, 0.0156],\n",
            "        [0.2133, 0.0000],\n",
            "        [0.2133, 0.0156],\n",
            "        [0.2133, 0.0156],\n",
            "        [0.2133, 0.0156],\n",
            "        [0.0167, 0.5117],\n",
            "        [0.0167, 0.5117],\n",
            "        [0.0333, 0.1172],\n",
            "        [0.0333, 0.1172],\n",
            "        [0.0373, 0.1172],\n",
            "        [0.0427, 0.1172],\n",
            "        [0.0333, 0.0312],\n",
            "        [0.0333, 0.0312],\n",
            "        [0.0333, 0.0312],\n",
            "        [0.0333, 0.0312],\n",
            "        [0.0333, 0.0312],\n",
            "        [0.0333, 0.0312],\n",
            "        [0.0333, 0.0312],\n",
            "        [0.0333, 0.0312],\n",
            "        [0.0887, 0.0352],\n",
            "        [0.0887, 0.0352],\n",
            "        [0.5400, 0.0312],\n",
            "        [0.5400, 0.0000],\n",
            "        [0.2133, 0.0156],\n",
            "        [0.1333, 0.0312],\n",
            "        [0.4667, 0.0000],\n",
            "        [0.4667, 0.0000],\n",
            "        [0.0933, 0.0625],\n",
            "        [0.1333, 0.0000],\n",
            "        [0.0733, 0.0625],\n",
            "        [0.0733, 0.0625],\n",
            "        [0.1467, 0.0625],\n",
            "        [0.5333, 0.0000],\n",
            "        [0.5333, 0.0000],\n",
            "        [0.5333, 0.0000],\n",
            "        [0.5333, 0.0000],\n",
            "        [0.5333, 0.0000],\n",
            "        [0.0833, 0.0000],\n",
            "        [0.0500, 0.2500],\n",
            "        [0.0500, 0.2500],\n",
            "        [0.0500, 0.5000],\n",
            "        [0.0600, 0.0000],\n",
            "        [0.0700, 0.0000],\n",
            "        [0.0700, 0.0000],\n",
            "        [0.0700, 0.0312],\n",
            "        [0.0500, 0.0312],\n",
            "        [0.0500, 0.0312],\n",
            "        [0.1167, 0.0000],\n",
            "        [0.2000, 0.0000],\n",
            "        [0.2000, 0.0234],\n",
            "        [0.2000, 0.0234],\n",
            "        [0.2000, 0.0000],\n",
            "        [0.2000, 0.0234],\n",
            "        [0.2000, 0.0234],\n",
            "        [0.1200, 0.0234],\n",
            "        [0.2200, 0.0000],\n",
            "        [0.2000, 0.0312],\n",
            "        [0.2000, 0.0312],\n",
            "        [0.2200, 0.0000],\n",
            "        [0.2200, 0.0000],\n",
            "        [0.0933, 0.0000],\n",
            "        [0.0933, 0.0000],\n",
            "        [0.0933, 0.0312],\n",
            "        [0.0933, 0.1250],\n",
            "        [0.0933, 0.1250],\n",
            "        [0.0933, 0.1250],\n",
            "        [0.0933, 0.1250],\n",
            "        [0.0933, 0.0312],\n",
            "        [0.0380, 0.0039],\n",
            "        [0.0380, 0.2500],\n",
            "        [0.0173, 0.2500],\n",
            "        [0.0173, 0.2500],\n",
            "        [0.0173, 0.0000],\n",
            "        [0.0173, 0.0000],\n",
            "        [0.3200, 0.0000],\n",
            "        [0.1353, 0.0000],\n",
            "        [0.0767, 0.0625],\n",
            "        [0.7333, 0.0000],\n",
            "        [0.7333, 0.0000],\n",
            "        [0.4000, 0.0000],\n",
            "        [0.2667, 0.0000],\n",
            "        [0.2667, 0.0000],\n",
            "        [0.6000, 0.0000],\n",
            "        [0.6000, 0.0000],\n",
            "        [0.6000, 0.0156],\n",
            "        [0.6000, 0.0312],\n",
            "        [0.6000, 0.0000],\n",
            "        [0.1500, 0.0312],\n",
            "        [0.1500, 0.0312],\n",
            "        [0.1200, 0.0312],\n",
            "        [0.1233, 0.0625],\n",
            "        [0.1200, 0.0625],\n",
            "        [0.1500, 0.0078],\n",
            "        [0.0167, 0.0312],\n",
            "        [0.0167, 0.0625],\n",
            "        [0.0113, 0.0312],\n",
            "        [0.0113, 0.1250],\n",
            "        [1.0000, 0.0000],\n",
            "        [1.0000, 0.0000],\n",
            "        [0.5333, 0.0000],\n",
            "        [0.0333, 0.0000],\n",
            "        [0.0333, 0.0312],\n",
            "        [0.0333, 0.0312],\n",
            "        [0.0333, 0.0938],\n",
            "        [0.0333, 0.0938],\n",
            "        [0.0333, 0.1875],\n",
            "        [0.0667, 0.0000],\n",
            "        [0.0667, 0.0938],\n",
            "        [0.0667, 0.0938],\n",
            "        [0.0333, 0.0469],\n",
            "        [0.0333, 0.0938],\n",
            "        [0.0333, 0.0938],\n",
            "        [0.1000, 0.0000],\n",
            "        [0.0767, 0.0625],\n",
            "        [0.0767, 0.0078],\n",
            "        [0.0613, 0.1250],\n",
            "        [0.0613, 0.1250],\n",
            "        [0.0613, 0.0156],\n",
            "        [0.0500, 0.0625],\n",
            "        [0.0400, 0.1250],\n",
            "        [0.0400, 0.2500],\n",
            "        [0.0400, 0.2500],\n",
            "        [0.0333, 0.2500],\n",
            "        [0.0480, 0.2500],\n",
            "        [0.0480, 0.0625],\n",
            "        [0.0267, 0.1250],\n",
            "        [0.0267, 0.2500],\n",
            "        [0.0233, 0.2500],\n",
            "        [0.0253, 0.5000],\n",
            "        [0.0320, 0.1250],\n",
            "        [0.0253, 0.2500],\n",
            "        [0.0200, 1.0000],\n",
            "        [0.0747, 0.0000],\n",
            "        [0.0560, 0.0000],\n",
            "        [0.0373, 0.0000],\n",
            "        [0.0373, 0.0000],\n",
            "        [0.0373, 0.0000],\n",
            "        [0.0373, 0.0000],\n",
            "        [0.0373, 0.0000],\n",
            "        [0.0373, 0.0000],\n",
            "        [0.0253, 0.1250],\n",
            "        [0.0253, 0.1250],\n",
            "        [0.0253, 0.2500],\n",
            "        [0.0253, 0.6250],\n",
            "        [0.0253, 0.5000],\n",
            "        [0.1333, 0.0000],\n",
            "        [0.1333, 0.0000],\n",
            "        [0.1333, 0.2500],\n",
            "        [0.1667, 0.0000],\n",
            "        [0.1667, 0.0000],\n",
            "        [0.1667, 0.0039],\n",
            "        [0.1067, 0.0078],\n",
            "        [0.1067, 0.0078],\n",
            "        [0.1067, 0.0312],\n",
            "        [0.1067, 0.0625],\n",
            "        [0.1067, 0.1250],\n",
            "        [0.1600, 0.0312],\n",
            "        [0.1600, 0.0312],\n",
            "        [0.0700, 0.0312],\n",
            "        [0.0700, 0.0625],\n",
            "        [0.0700, 0.0625],\n",
            "        [0.0347, 0.1250],\n",
            "        [0.0467, 0.0312],\n",
            "        [0.0393, 0.1250],\n",
            "        [0.0393, 0.2500],\n",
            "        [0.0173, 0.1250],\n",
            "        [0.0173, 0.2500],\n",
            "        [0.0173, 0.5000],\n",
            "        [0.0773, 0.1250],\n",
            "        [0.0333, 0.0938],\n",
            "        [0.0333, 0.1875],\n",
            "        [0.0333, 0.4375],\n",
            "        [0.0333, 0.4375],\n",
            "        [0.0200, 0.3750],\n",
            "        [0.0200, 0.5000],\n",
            "        [0.1200, 0.0000],\n",
            "        [0.1200, 0.0000],\n",
            "        [0.1200, 0.0000],\n",
            "        [0.1200, 0.0000],\n",
            "        [0.0827, 0.0000],\n",
            "        [0.0653, 0.1250],\n",
            "        [0.0833, 0.0000],\n",
            "        [0.3200, 0.1250],\n",
            "        [0.3200, 0.0000]])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([209])) that is different to the input size (torch.Size([209, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1000/2000000], Loss: 0.03868767\n",
            "Epoch [2000/2000000], Loss: 0.03864184\n",
            "Epoch [3000/2000000], Loss: 0.03859605\n",
            "Epoch [4000/2000000], Loss: 0.03855028\n",
            "Epoch [5000/2000000], Loss: 0.03850454\n",
            "Epoch [6000/2000000], Loss: 0.03845884\n",
            "Epoch [7000/2000000], Loss: 0.03841316\n",
            "Epoch [8000/2000000], Loss: 0.03836751\n",
            "Epoch [9000/2000000], Loss: 0.03832189\n",
            "Epoch [10000/2000000], Loss: 0.03827630\n",
            "Epoch [11000/2000000], Loss: 0.03823075\n",
            "Epoch [12000/2000000], Loss: 0.03818525\n",
            "Epoch [13000/2000000], Loss: 0.03813977\n",
            "Epoch [14000/2000000], Loss: 0.03809432\n",
            "Epoch [15000/2000000], Loss: 0.03804890\n",
            "Epoch [16000/2000000], Loss: 0.03800353\n",
            "Epoch [17000/2000000], Loss: 0.03795821\n",
            "Epoch [18000/2000000], Loss: 0.03791292\n",
            "Epoch [19000/2000000], Loss: 0.03786767\n",
            "Epoch [20000/2000000], Loss: 0.03782243\n",
            "Epoch [21000/2000000], Loss: 0.03777723\n",
            "Epoch [22000/2000000], Loss: 0.03773206\n",
            "Epoch [23000/2000000], Loss: 0.03768692\n",
            "Epoch [24000/2000000], Loss: 0.03764180\n",
            "Epoch [25000/2000000], Loss: 0.03759672\n",
            "Epoch [26000/2000000], Loss: 0.03755166\n",
            "Epoch [27000/2000000], Loss: 0.03750665\n",
            "Epoch [28000/2000000], Loss: 0.03746168\n",
            "Epoch [29000/2000000], Loss: 0.03741673\n",
            "Epoch [30000/2000000], Loss: 0.03737181\n",
            "Epoch [31000/2000000], Loss: 0.03732693\n",
            "Epoch [32000/2000000], Loss: 0.03728207\n",
            "Epoch [33000/2000000], Loss: 0.03723724\n",
            "Epoch [34000/2000000], Loss: 0.03719245\n",
            "Epoch [35000/2000000], Loss: 0.03714767\n",
            "Epoch [36000/2000000], Loss: 0.03710296\n",
            "Epoch [37000/2000000], Loss: 0.03705834\n",
            "Epoch [38000/2000000], Loss: 0.03701374\n",
            "Epoch [39000/2000000], Loss: 0.03696917\n",
            "Epoch [40000/2000000], Loss: 0.03692464\n",
            "Epoch [41000/2000000], Loss: 0.03688012\n",
            "Epoch [42000/2000000], Loss: 0.03683565\n",
            "Epoch [43000/2000000], Loss: 0.03679120\n",
            "Epoch [44000/2000000], Loss: 0.03674678\n",
            "Epoch [45000/2000000], Loss: 0.03670239\n",
            "Epoch [46000/2000000], Loss: 0.03665803\n",
            "Epoch [47000/2000000], Loss: 0.03661370\n",
            "Epoch [48000/2000000], Loss: 0.03656939\n",
            "Epoch [49000/2000000], Loss: 0.03652512\n",
            "Epoch [50000/2000000], Loss: 0.03648087\n",
            "Epoch [51000/2000000], Loss: 0.03643666\n",
            "Epoch [52000/2000000], Loss: 0.03639248\n",
            "Epoch [53000/2000000], Loss: 0.03634832\n",
            "Epoch [54000/2000000], Loss: 0.03630419\n",
            "Epoch [55000/2000000], Loss: 0.03626012\n",
            "Epoch [56000/2000000], Loss: 0.03621608\n",
            "Epoch [57000/2000000], Loss: 0.03617207\n",
            "Epoch [58000/2000000], Loss: 0.03612810\n",
            "Epoch [59000/2000000], Loss: 0.03608417\n",
            "Epoch [60000/2000000], Loss: 0.03604027\n",
            "Epoch [61000/2000000], Loss: 0.03599641\n",
            "Epoch [62000/2000000], Loss: 0.03595256\n",
            "Epoch [63000/2000000], Loss: 0.03590876\n",
            "Epoch [64000/2000000], Loss: 0.03586498\n",
            "Epoch [65000/2000000], Loss: 0.03582123\n",
            "Epoch [66000/2000000], Loss: 0.03577754\n",
            "Epoch [67000/2000000], Loss: 0.03573388\n",
            "Epoch [68000/2000000], Loss: 0.03569026\n",
            "Epoch [69000/2000000], Loss: 0.03564666\n",
            "Epoch [70000/2000000], Loss: 0.03560309\n",
            "Epoch [71000/2000000], Loss: 0.03555955\n",
            "Epoch [72000/2000000], Loss: 0.03551604\n",
            "Epoch [73000/2000000], Loss: 0.03547256\n",
            "Epoch [74000/2000000], Loss: 0.03542914\n",
            "Epoch [75000/2000000], Loss: 0.03538578\n",
            "Epoch [76000/2000000], Loss: 0.03534245\n",
            "Epoch [77000/2000000], Loss: 0.03529914\n",
            "Epoch [78000/2000000], Loss: 0.03525587\n",
            "Epoch [79000/2000000], Loss: 0.03521262\n",
            "Epoch [80000/2000000], Loss: 0.03516941\n",
            "Epoch [81000/2000000], Loss: 0.03512622\n",
            "Epoch [82000/2000000], Loss: 0.03508307\n",
            "Epoch [83000/2000000], Loss: 0.03503994\n",
            "Epoch [84000/2000000], Loss: 0.03499683\n",
            "Epoch [85000/2000000], Loss: 0.03495377\n",
            "Epoch [86000/2000000], Loss: 0.03491072\n",
            "Epoch [87000/2000000], Loss: 0.03486772\n",
            "Epoch [88000/2000000], Loss: 0.03482473\n",
            "Epoch [89000/2000000], Loss: 0.03478178\n",
            "Epoch [90000/2000000], Loss: 0.03473885\n",
            "Epoch [91000/2000000], Loss: 0.03469596\n",
            "Epoch [92000/2000000], Loss: 0.03465309\n",
            "Epoch [93000/2000000], Loss: 0.03461026\n",
            "Epoch [94000/2000000], Loss: 0.03456746\n",
            "Epoch [95000/2000000], Loss: 0.03452470\n",
            "Epoch [96000/2000000], Loss: 0.03448197\n",
            "Epoch [97000/2000000], Loss: 0.03443927\n",
            "Epoch [98000/2000000], Loss: 0.03439659\n",
            "Epoch [99000/2000000], Loss: 0.03435393\n",
            "Epoch [100000/2000000], Loss: 0.03431132\n",
            "Epoch [101000/2000000], Loss: 0.03426873\n",
            "Epoch [102000/2000000], Loss: 0.03422616\n",
            "Epoch [103000/2000000], Loss: 0.03418363\n",
            "Epoch [104000/2000000], Loss: 0.03414113\n",
            "Epoch [105000/2000000], Loss: 0.03409865\n",
            "Epoch [106000/2000000], Loss: 0.03405621\n",
            "Epoch [107000/2000000], Loss: 0.03401380\n",
            "Epoch [108000/2000000], Loss: 0.03397142\n",
            "Epoch [109000/2000000], Loss: 0.03392907\n",
            "Epoch [110000/2000000], Loss: 0.03388674\n",
            "Epoch [111000/2000000], Loss: 0.03384447\n",
            "Epoch [112000/2000000], Loss: 0.03380222\n",
            "Epoch [113000/2000000], Loss: 0.03376000\n",
            "Epoch [114000/2000000], Loss: 0.03371782\n",
            "Epoch [115000/2000000], Loss: 0.03367566\n",
            "Epoch [116000/2000000], Loss: 0.03363353\n",
            "Epoch [117000/2000000], Loss: 0.03359143\n",
            "Epoch [118000/2000000], Loss: 0.03354936\n",
            "Epoch [119000/2000000], Loss: 0.03350732\n",
            "Epoch [120000/2000000], Loss: 0.03346531\n",
            "Epoch [121000/2000000], Loss: 0.03342332\n",
            "Epoch [122000/2000000], Loss: 0.03338137\n",
            "Epoch [123000/2000000], Loss: 0.03333944\n",
            "Epoch [124000/2000000], Loss: 0.03329755\n",
            "Epoch [125000/2000000], Loss: 0.03325568\n",
            "Epoch [126000/2000000], Loss: 0.03321385\n",
            "Epoch [127000/2000000], Loss: 0.03317203\n",
            "Epoch [128000/2000000], Loss: 0.03313025\n",
            "Epoch [129000/2000000], Loss: 0.03308850\n",
            "Epoch [130000/2000000], Loss: 0.03304678\n",
            "Epoch [131000/2000000], Loss: 0.03300509\n",
            "Epoch [132000/2000000], Loss: 0.03296342\n",
            "Epoch [133000/2000000], Loss: 0.03292179\n",
            "Epoch [134000/2000000], Loss: 0.03288019\n",
            "Epoch [135000/2000000], Loss: 0.03283861\n",
            "Epoch [136000/2000000], Loss: 0.03279706\n",
            "Epoch [137000/2000000], Loss: 0.03275554\n",
            "Epoch [138000/2000000], Loss: 0.03271405\n",
            "Epoch [139000/2000000], Loss: 0.03267259\n",
            "Epoch [140000/2000000], Loss: 0.03263116\n",
            "Epoch [141000/2000000], Loss: 0.03258976\n",
            "Epoch [142000/2000000], Loss: 0.03254839\n",
            "Epoch [143000/2000000], Loss: 0.03250704\n",
            "Epoch [144000/2000000], Loss: 0.03246573\n",
            "Epoch [145000/2000000], Loss: 0.03242445\n",
            "Epoch [146000/2000000], Loss: 0.03238319\n",
            "Epoch [147000/2000000], Loss: 0.03234196\n",
            "Epoch [148000/2000000], Loss: 0.03230076\n",
            "Epoch [149000/2000000], Loss: 0.03225959\n",
            "Epoch [150000/2000000], Loss: 0.03221845\n",
            "Epoch [151000/2000000], Loss: 0.03217734\n",
            "Epoch [152000/2000000], Loss: 0.03213626\n",
            "Epoch [153000/2000000], Loss: 0.03209521\n",
            "Epoch [154000/2000000], Loss: 0.03205419\n",
            "Epoch [155000/2000000], Loss: 0.03201319\n",
            "Epoch [156000/2000000], Loss: 0.03197223\n",
            "Epoch [157000/2000000], Loss: 0.03193129\n",
            "Epoch [158000/2000000], Loss: 0.03189038\n",
            "Epoch [159000/2000000], Loss: 0.03184951\n",
            "Epoch [160000/2000000], Loss: 0.03180866\n",
            "Epoch [161000/2000000], Loss: 0.03176784\n",
            "Epoch [162000/2000000], Loss: 0.03172705\n",
            "Epoch [163000/2000000], Loss: 0.03168629\n",
            "Epoch [164000/2000000], Loss: 0.03164557\n",
            "Epoch [165000/2000000], Loss: 0.03160487\n",
            "Epoch [166000/2000000], Loss: 0.03156419\n",
            "Epoch [167000/2000000], Loss: 0.03152356\n",
            "Epoch [168000/2000000], Loss: 0.03148294\n",
            "Epoch [169000/2000000], Loss: 0.03144236\n",
            "Epoch [170000/2000000], Loss: 0.03140181\n",
            "Epoch [171000/2000000], Loss: 0.03136129\n",
            "Epoch [172000/2000000], Loss: 0.03132079\n",
            "Epoch [173000/2000000], Loss: 0.03128033\n",
            "Epoch [174000/2000000], Loss: 0.03123989\n",
            "Epoch [175000/2000000], Loss: 0.03119949\n",
            "Epoch [176000/2000000], Loss: 0.03115911\n",
            "Epoch [177000/2000000], Loss: 0.03111876\n",
            "Epoch [178000/2000000], Loss: 0.03107844\n",
            "Epoch [179000/2000000], Loss: 0.03103815\n",
            "Epoch [180000/2000000], Loss: 0.03099789\n",
            "Epoch [181000/2000000], Loss: 0.03095766\n",
            "Epoch [182000/2000000], Loss: 0.03091745\n",
            "Epoch [183000/2000000], Loss: 0.03087729\n",
            "Epoch [184000/2000000], Loss: 0.03083714\n",
            "Epoch [185000/2000000], Loss: 0.03079703\n",
            "Epoch [186000/2000000], Loss: 0.03075694\n",
            "Epoch [187000/2000000], Loss: 0.03071688\n",
            "Epoch [188000/2000000], Loss: 0.03067686\n",
            "Epoch [189000/2000000], Loss: 0.03063687\n",
            "Epoch [190000/2000000], Loss: 0.03059689\n",
            "Epoch [191000/2000000], Loss: 0.03055695\n",
            "Epoch [192000/2000000], Loss: 0.03051706\n",
            "Epoch [193000/2000000], Loss: 0.03047722\n",
            "Epoch [194000/2000000], Loss: 0.03043741\n",
            "Epoch [195000/2000000], Loss: 0.03039762\n",
            "Epoch [196000/2000000], Loss: 0.03035787\n",
            "Epoch [197000/2000000], Loss: 0.03031813\n",
            "Epoch [198000/2000000], Loss: 0.03027839\n",
            "Epoch [199000/2000000], Loss: 0.03023869\n",
            "Epoch [200000/2000000], Loss: 0.03019902\n",
            "Epoch [201000/2000000], Loss: 0.03015937\n",
            "Epoch [202000/2000000], Loss: 0.03011976\n",
            "Epoch [203000/2000000], Loss: 0.03008018\n",
            "Epoch [204000/2000000], Loss: 0.03004062\n",
            "Epoch [205000/2000000], Loss: 0.03000110\n",
            "Epoch [206000/2000000], Loss: 0.02996160\n",
            "Epoch [207000/2000000], Loss: 0.02992213\n",
            "Epoch [208000/2000000], Loss: 0.02988269\n",
            "Epoch [209000/2000000], Loss: 0.02984328\n",
            "Epoch [210000/2000000], Loss: 0.02980390\n",
            "Epoch [211000/2000000], Loss: 0.02976455\n",
            "Epoch [212000/2000000], Loss: 0.02972523\n",
            "Epoch [213000/2000000], Loss: 0.02968594\n",
            "Epoch [214000/2000000], Loss: 0.02964667\n",
            "Epoch [215000/2000000], Loss: 0.02960744\n",
            "Epoch [216000/2000000], Loss: 0.02956824\n",
            "Epoch [217000/2000000], Loss: 0.02952907\n",
            "Epoch [218000/2000000], Loss: 0.02948993\n",
            "Epoch [219000/2000000], Loss: 0.02945082\n",
            "Epoch [220000/2000000], Loss: 0.02941174\n",
            "Epoch [221000/2000000], Loss: 0.02937268\n",
            "Epoch [222000/2000000], Loss: 0.02933365\n",
            "Epoch [223000/2000000], Loss: 0.02929466\n",
            "Epoch [224000/2000000], Loss: 0.02925574\n",
            "Epoch [225000/2000000], Loss: 0.02921688\n",
            "Epoch [226000/2000000], Loss: 0.02917805\n",
            "Epoch [227000/2000000], Loss: 0.02913926\n",
            "Epoch [228000/2000000], Loss: 0.02910049\n",
            "Epoch [229000/2000000], Loss: 0.02906175\n",
            "Epoch [230000/2000000], Loss: 0.02902304\n",
            "Epoch [231000/2000000], Loss: 0.02898436\n",
            "Epoch [232000/2000000], Loss: 0.02894570\n",
            "Epoch [233000/2000000], Loss: 0.02890708\n",
            "Epoch [234000/2000000], Loss: 0.02886849\n",
            "Epoch [235000/2000000], Loss: 0.02882992\n",
            "Epoch [236000/2000000], Loss: 0.02879138\n",
            "Epoch [237000/2000000], Loss: 0.02875288\n",
            "Epoch [238000/2000000], Loss: 0.02871440\n",
            "Epoch [239000/2000000], Loss: 0.02867595\n",
            "Epoch [240000/2000000], Loss: 0.02863753\n",
            "Epoch [241000/2000000], Loss: 0.02859914\n",
            "Epoch [242000/2000000], Loss: 0.02856077\n",
            "Epoch [243000/2000000], Loss: 0.02852244\n",
            "Epoch [244000/2000000], Loss: 0.02848413\n",
            "Epoch [245000/2000000], Loss: 0.02844586\n",
            "Epoch [246000/2000000], Loss: 0.02840761\n",
            "Epoch [247000/2000000], Loss: 0.02836939\n",
            "Epoch [248000/2000000], Loss: 0.02833120\n",
            "Epoch [249000/2000000], Loss: 0.02829304\n",
            "Epoch [250000/2000000], Loss: 0.02825491\n",
            "Epoch [251000/2000000], Loss: 0.02821681\n",
            "Epoch [252000/2000000], Loss: 0.02817874\n",
            "Epoch [253000/2000000], Loss: 0.02814072\n",
            "Epoch [254000/2000000], Loss: 0.02810272\n",
            "Epoch [255000/2000000], Loss: 0.02806475\n",
            "Epoch [256000/2000000], Loss: 0.02802682\n",
            "Epoch [257000/2000000], Loss: 0.02798893\n",
            "Epoch [258000/2000000], Loss: 0.02795107\n",
            "Epoch [259000/2000000], Loss: 0.02791325\n",
            "Epoch [260000/2000000], Loss: 0.02787545\n",
            "Epoch [261000/2000000], Loss: 0.02783768\n",
            "Epoch [262000/2000000], Loss: 0.02779994\n",
            "Epoch [263000/2000000], Loss: 0.02776222\n",
            "Epoch [264000/2000000], Loss: 0.02772454\n",
            "Epoch [265000/2000000], Loss: 0.02768689\n",
            "Epoch [266000/2000000], Loss: 0.02764926\n",
            "Epoch [267000/2000000], Loss: 0.02761167\n",
            "Epoch [268000/2000000], Loss: 0.02757410\n",
            "Epoch [269000/2000000], Loss: 0.02753656\n",
            "Epoch [270000/2000000], Loss: 0.02749908\n",
            "Epoch [271000/2000000], Loss: 0.02746163\n",
            "Epoch [272000/2000000], Loss: 0.02742421\n",
            "Epoch [273000/2000000], Loss: 0.02738682\n",
            "Epoch [274000/2000000], Loss: 0.02734945\n",
            "Epoch [275000/2000000], Loss: 0.02731212\n",
            "Epoch [276000/2000000], Loss: 0.02727481\n",
            "Epoch [277000/2000000], Loss: 0.02723754\n",
            "Epoch [278000/2000000], Loss: 0.02720036\n",
            "Epoch [279000/2000000], Loss: 0.02716321\n",
            "Epoch [280000/2000000], Loss: 0.02712609\n",
            "Epoch [281000/2000000], Loss: 0.02708900\n",
            "Epoch [282000/2000000], Loss: 0.02705194\n",
            "Epoch [283000/2000000], Loss: 0.02701491\n",
            "Epoch [284000/2000000], Loss: 0.02697790\n",
            "Epoch [285000/2000000], Loss: 0.02694093\n",
            "Epoch [286000/2000000], Loss: 0.02690398\n",
            "Epoch [287000/2000000], Loss: 0.02686706\n",
            "Epoch [288000/2000000], Loss: 0.02683017\n",
            "Epoch [289000/2000000], Loss: 0.02679331\n",
            "Epoch [290000/2000000], Loss: 0.02675648\n",
            "Epoch [291000/2000000], Loss: 0.02671968\n",
            "Epoch [292000/2000000], Loss: 0.02668290\n",
            "Epoch [293000/2000000], Loss: 0.02664616\n",
            "Epoch [294000/2000000], Loss: 0.02660944\n",
            "Epoch [295000/2000000], Loss: 0.02657275\n",
            "Epoch [296000/2000000], Loss: 0.02653610\n",
            "Epoch [297000/2000000], Loss: 0.02649947\n",
            "Epoch [298000/2000000], Loss: 0.02646287\n",
            "Epoch [299000/2000000], Loss: 0.02642629\n",
            "Epoch [300000/2000000], Loss: 0.02638975\n",
            "Epoch [301000/2000000], Loss: 0.02635324\n",
            "Epoch [302000/2000000], Loss: 0.02631675\n",
            "Epoch [303000/2000000], Loss: 0.02628030\n",
            "Epoch [304000/2000000], Loss: 0.02624388\n",
            "Epoch [305000/2000000], Loss: 0.02620750\n",
            "Epoch [306000/2000000], Loss: 0.02617115\n",
            "Epoch [307000/2000000], Loss: 0.02613483\n",
            "Epoch [308000/2000000], Loss: 0.02609853\n",
            "Epoch [309000/2000000], Loss: 0.02606226\n",
            "Epoch [310000/2000000], Loss: 0.02602602\n",
            "Epoch [311000/2000000], Loss: 0.02598982\n",
            "Epoch [312000/2000000], Loss: 0.02595364\n",
            "Epoch [313000/2000000], Loss: 0.02591748\n",
            "Epoch [314000/2000000], Loss: 0.02588136\n",
            "Epoch [315000/2000000], Loss: 0.02584527\n",
            "Epoch [316000/2000000], Loss: 0.02580920\n",
            "Epoch [317000/2000000], Loss: 0.02577317\n",
            "Epoch [318000/2000000], Loss: 0.02573716\n",
            "Epoch [319000/2000000], Loss: 0.02570118\n",
            "Epoch [320000/2000000], Loss: 0.02566523\n",
            "Epoch [321000/2000000], Loss: 0.02562931\n",
            "Epoch [322000/2000000], Loss: 0.02559341\n",
            "Epoch [323000/2000000], Loss: 0.02555755\n",
            "Epoch [324000/2000000], Loss: 0.02552171\n",
            "Epoch [325000/2000000], Loss: 0.02548591\n",
            "Epoch [326000/2000000], Loss: 0.02545013\n",
            "Epoch [327000/2000000], Loss: 0.02541438\n",
            "Epoch [328000/2000000], Loss: 0.02537867\n",
            "Epoch [329000/2000000], Loss: 0.02534298\n",
            "Epoch [330000/2000000], Loss: 0.02530732\n",
            "Epoch [331000/2000000], Loss: 0.02527183\n",
            "Epoch [332000/2000000], Loss: 0.02523639\n",
            "Epoch [333000/2000000], Loss: 0.02520097\n",
            "Epoch [334000/2000000], Loss: 0.02516558\n",
            "Epoch [335000/2000000], Loss: 0.02513022\n",
            "Epoch [336000/2000000], Loss: 0.02509488\n",
            "Epoch [337000/2000000], Loss: 0.02505957\n",
            "Epoch [338000/2000000], Loss: 0.02502429\n",
            "Epoch [339000/2000000], Loss: 0.02498904\n",
            "Epoch [340000/2000000], Loss: 0.02495382\n",
            "Epoch [341000/2000000], Loss: 0.02491863\n",
            "Epoch [342000/2000000], Loss: 0.02488346\n",
            "Epoch [343000/2000000], Loss: 0.02484833\n",
            "Epoch [344000/2000000], Loss: 0.02481322\n",
            "Epoch [345000/2000000], Loss: 0.02477814\n",
            "Epoch [346000/2000000], Loss: 0.02474309\n",
            "Epoch [347000/2000000], Loss: 0.02470807\n",
            "Epoch [348000/2000000], Loss: 0.02467308\n",
            "Epoch [349000/2000000], Loss: 0.02463812\n",
            "Epoch [350000/2000000], Loss: 0.02460318\n",
            "Epoch [351000/2000000], Loss: 0.02456827\n",
            "Epoch [352000/2000000], Loss: 0.02453338\n",
            "Epoch [353000/2000000], Loss: 0.02449852\n",
            "Epoch [354000/2000000], Loss: 0.02446369\n",
            "Epoch [355000/2000000], Loss: 0.02442889\n",
            "Epoch [356000/2000000], Loss: 0.02439412\n",
            "Epoch [357000/2000000], Loss: 0.02435939\n",
            "Epoch [358000/2000000], Loss: 0.02432471\n",
            "Epoch [359000/2000000], Loss: 0.02429005\n",
            "Epoch [360000/2000000], Loss: 0.02425542\n",
            "Epoch [361000/2000000], Loss: 0.02422083\n",
            "Epoch [362000/2000000], Loss: 0.02418625\n",
            "Epoch [363000/2000000], Loss: 0.02415171\n",
            "Epoch [364000/2000000], Loss: 0.02411719\n",
            "Epoch [365000/2000000], Loss: 0.02408271\n",
            "Epoch [366000/2000000], Loss: 0.02404826\n",
            "Epoch [367000/2000000], Loss: 0.02401383\n",
            "Epoch [368000/2000000], Loss: 0.02397943\n",
            "Epoch [369000/2000000], Loss: 0.02394506\n",
            "Epoch [370000/2000000], Loss: 0.02391073\n",
            "Epoch [371000/2000000], Loss: 0.02387643\n",
            "Epoch [372000/2000000], Loss: 0.02384217\n",
            "Epoch [373000/2000000], Loss: 0.02380792\n",
            "Epoch [374000/2000000], Loss: 0.02377372\n",
            "Epoch [375000/2000000], Loss: 0.02373954\n",
            "Epoch [376000/2000000], Loss: 0.02370539\n",
            "Epoch [377000/2000000], Loss: 0.02367128\n",
            "Epoch [378000/2000000], Loss: 0.02363718\n",
            "Epoch [379000/2000000], Loss: 0.02360312\n",
            "Epoch [380000/2000000], Loss: 0.02356912\n",
            "Epoch [381000/2000000], Loss: 0.02353515\n",
            "Epoch [382000/2000000], Loss: 0.02350120\n",
            "Epoch [383000/2000000], Loss: 0.02346728\n",
            "Epoch [384000/2000000], Loss: 0.02343339\n",
            "Epoch [385000/2000000], Loss: 0.02339953\n",
            "Epoch [386000/2000000], Loss: 0.02336570\n",
            "Epoch [387000/2000000], Loss: 0.02333190\n",
            "Epoch [388000/2000000], Loss: 0.02329812\n",
            "Epoch [389000/2000000], Loss: 0.02326438\n",
            "Epoch [390000/2000000], Loss: 0.02323066\n",
            "Epoch [391000/2000000], Loss: 0.02319697\n",
            "Epoch [392000/2000000], Loss: 0.02316331\n",
            "Epoch [393000/2000000], Loss: 0.02312968\n",
            "Epoch [394000/2000000], Loss: 0.02309608\n",
            "Epoch [395000/2000000], Loss: 0.02306250\n",
            "Epoch [396000/2000000], Loss: 0.02302896\n",
            "Epoch [397000/2000000], Loss: 0.02299546\n",
            "Epoch [398000/2000000], Loss: 0.02296199\n",
            "Epoch [399000/2000000], Loss: 0.02292855\n",
            "Epoch [400000/2000000], Loss: 0.02289514\n",
            "Epoch [401000/2000000], Loss: 0.02286176\n",
            "Epoch [402000/2000000], Loss: 0.02282840\n",
            "Epoch [403000/2000000], Loss: 0.02279508\n",
            "Epoch [404000/2000000], Loss: 0.02276178\n",
            "Epoch [405000/2000000], Loss: 0.02272851\n",
            "Epoch [406000/2000000], Loss: 0.02269527\n",
            "Epoch [407000/2000000], Loss: 0.02266206\n",
            "Epoch [408000/2000000], Loss: 0.02262888\n",
            "Epoch [409000/2000000], Loss: 0.02259572\n",
            "Epoch [410000/2000000], Loss: 0.02256260\n",
            "Epoch [411000/2000000], Loss: 0.02252950\n",
            "Epoch [412000/2000000], Loss: 0.02249644\n",
            "Epoch [413000/2000000], Loss: 0.02246340\n",
            "Epoch [414000/2000000], Loss: 0.02243038\n",
            "Epoch [415000/2000000], Loss: 0.02239740\n",
            "Epoch [416000/2000000], Loss: 0.02236445\n",
            "Epoch [417000/2000000], Loss: 0.02233153\n",
            "Epoch [418000/2000000], Loss: 0.02229863\n",
            "Epoch [419000/2000000], Loss: 0.02226582\n",
            "Epoch [420000/2000000], Loss: 0.02223330\n",
            "Epoch [421000/2000000], Loss: 0.02220081\n",
            "Epoch [422000/2000000], Loss: 0.02216836\n",
            "Epoch [423000/2000000], Loss: 0.02213593\n",
            "Epoch [424000/2000000], Loss: 0.02210353\n",
            "Epoch [425000/2000000], Loss: 0.02207116\n",
            "Epoch [426000/2000000], Loss: 0.02203882\n",
            "Epoch [427000/2000000], Loss: 0.02200653\n",
            "Epoch [428000/2000000], Loss: 0.02197430\n",
            "Epoch [429000/2000000], Loss: 0.02194211\n",
            "Epoch [430000/2000000], Loss: 0.02190995\n",
            "Epoch [431000/2000000], Loss: 0.02187781\n",
            "Epoch [432000/2000000], Loss: 0.02184570\n",
            "Epoch [433000/2000000], Loss: 0.02181362\n",
            "Epoch [434000/2000000], Loss: 0.02178157\n",
            "Epoch [435000/2000000], Loss: 0.02174955\n",
            "Epoch [436000/2000000], Loss: 0.02171756\n",
            "Epoch [437000/2000000], Loss: 0.02168559\n",
            "Epoch [438000/2000000], Loss: 0.02165365\n",
            "Epoch [439000/2000000], Loss: 0.02162174\n",
            "Epoch [440000/2000000], Loss: 0.02158986\n",
            "Epoch [441000/2000000], Loss: 0.02155801\n",
            "Epoch [442000/2000000], Loss: 0.02152618\n",
            "Epoch [443000/2000000], Loss: 0.02149438\n",
            "Epoch [444000/2000000], Loss: 0.02146261\n",
            "Epoch [445000/2000000], Loss: 0.02143088\n",
            "Epoch [446000/2000000], Loss: 0.02139919\n",
            "Epoch [447000/2000000], Loss: 0.02136753\n",
            "Epoch [448000/2000000], Loss: 0.02133589\n",
            "Epoch [449000/2000000], Loss: 0.02130428\n",
            "Epoch [450000/2000000], Loss: 0.02127270\n",
            "Epoch [451000/2000000], Loss: 0.02124115\n",
            "Epoch [452000/2000000], Loss: 0.02120962\n",
            "Epoch [453000/2000000], Loss: 0.02117812\n",
            "Epoch [454000/2000000], Loss: 0.02114665\n",
            "Epoch [455000/2000000], Loss: 0.02111521\n",
            "Epoch [456000/2000000], Loss: 0.02108380\n",
            "Epoch [457000/2000000], Loss: 0.02105241\n",
            "Epoch [458000/2000000], Loss: 0.02102105\n",
            "Epoch [459000/2000000], Loss: 0.02098972\n",
            "Epoch [460000/2000000], Loss: 0.02095842\n",
            "Epoch [461000/2000000], Loss: 0.02092714\n",
            "Epoch [462000/2000000], Loss: 0.02089590\n",
            "Epoch [463000/2000000], Loss: 0.02086467\n",
            "Epoch [464000/2000000], Loss: 0.02083348\n",
            "Epoch [465000/2000000], Loss: 0.02080232\n",
            "Epoch [466000/2000000], Loss: 0.02077119\n",
            "Epoch [467000/2000000], Loss: 0.02074008\n",
            "Epoch [468000/2000000], Loss: 0.02070900\n",
            "Epoch [469000/2000000], Loss: 0.02067795\n",
            "Epoch [470000/2000000], Loss: 0.02064693\n",
            "Epoch [471000/2000000], Loss: 0.02061593\n",
            "Epoch [472000/2000000], Loss: 0.02058497\n",
            "Epoch [473000/2000000], Loss: 0.02055402\n",
            "Epoch [474000/2000000], Loss: 0.02052311\n",
            "Epoch [475000/2000000], Loss: 0.02049223\n",
            "Epoch [476000/2000000], Loss: 0.02046137\n",
            "Epoch [477000/2000000], Loss: 0.02043055\n",
            "Epoch [478000/2000000], Loss: 0.02039975\n",
            "Epoch [479000/2000000], Loss: 0.02036897\n",
            "Epoch [480000/2000000], Loss: 0.02033823\n",
            "Epoch [481000/2000000], Loss: 0.02030752\n",
            "Epoch [482000/2000000], Loss: 0.02027683\n",
            "Epoch [483000/2000000], Loss: 0.02024617\n",
            "Epoch [484000/2000000], Loss: 0.02021554\n",
            "Epoch [485000/2000000], Loss: 0.02018494\n",
            "Epoch [486000/2000000], Loss: 0.02015437\n",
            "Epoch [487000/2000000], Loss: 0.02012382\n",
            "Epoch [488000/2000000], Loss: 0.02009330\n",
            "Epoch [489000/2000000], Loss: 0.02006281\n",
            "Epoch [490000/2000000], Loss: 0.02003235\n",
            "Epoch [491000/2000000], Loss: 0.02000191\n",
            "Epoch [492000/2000000], Loss: 0.01997151\n",
            "Epoch [493000/2000000], Loss: 0.01994113\n",
            "Epoch [494000/2000000], Loss: 0.01991078\n",
            "Epoch [495000/2000000], Loss: 0.01988045\n",
            "Epoch [496000/2000000], Loss: 0.01985016\n",
            "Epoch [497000/2000000], Loss: 0.01981989\n",
            "Epoch [498000/2000000], Loss: 0.01978965\n",
            "Epoch [499000/2000000], Loss: 0.01975944\n",
            "Epoch [500000/2000000], Loss: 0.01972925\n",
            "Epoch [501000/2000000], Loss: 0.01969910\n",
            "Epoch [502000/2000000], Loss: 0.01966897\n",
            "Epoch [503000/2000000], Loss: 0.01963887\n",
            "Epoch [504000/2000000], Loss: 0.01960880\n",
            "Epoch [505000/2000000], Loss: 0.01957875\n",
            "Epoch [506000/2000000], Loss: 0.01954876\n",
            "Epoch [507000/2000000], Loss: 0.01951880\n",
            "Epoch [508000/2000000], Loss: 0.01948887\n",
            "Epoch [509000/2000000], Loss: 0.01945896\n",
            "Epoch [510000/2000000], Loss: 0.01942909\n",
            "Epoch [511000/2000000], Loss: 0.01939924\n",
            "Epoch [512000/2000000], Loss: 0.01936942\n",
            "Epoch [513000/2000000], Loss: 0.01933962\n",
            "Epoch [514000/2000000], Loss: 0.01930986\n",
            "Epoch [515000/2000000], Loss: 0.01928013\n",
            "Epoch [516000/2000000], Loss: 0.01925042\n",
            "Epoch [517000/2000000], Loss: 0.01922074\n",
            "Epoch [518000/2000000], Loss: 0.01919108\n",
            "Epoch [519000/2000000], Loss: 0.01916145\n",
            "Epoch [520000/2000000], Loss: 0.01913185\n",
            "Epoch [521000/2000000], Loss: 0.01910228\n",
            "Epoch [522000/2000000], Loss: 0.01907273\n",
            "Epoch [523000/2000000], Loss: 0.01904323\n",
            "Epoch [524000/2000000], Loss: 0.01901376\n",
            "Epoch [525000/2000000], Loss: 0.01898432\n",
            "Epoch [526000/2000000], Loss: 0.01895490\n",
            "Epoch [527000/2000000], Loss: 0.01892551\n",
            "Epoch [528000/2000000], Loss: 0.01889614\n",
            "Epoch [529000/2000000], Loss: 0.01886680\n",
            "Epoch [530000/2000000], Loss: 0.01883749\n",
            "Epoch [531000/2000000], Loss: 0.01880820\n",
            "Epoch [532000/2000000], Loss: 0.01877895\n",
            "Epoch [533000/2000000], Loss: 0.01874972\n",
            "Epoch [534000/2000000], Loss: 0.01872052\n",
            "Epoch [535000/2000000], Loss: 0.01869135\n",
            "Epoch [536000/2000000], Loss: 0.01866220\n",
            "Epoch [537000/2000000], Loss: 0.01863308\n",
            "Epoch [538000/2000000], Loss: 0.01860400\n",
            "Epoch [539000/2000000], Loss: 0.01857496\n",
            "Epoch [540000/2000000], Loss: 0.01854595\n",
            "Epoch [541000/2000000], Loss: 0.01851703\n",
            "Epoch [542000/2000000], Loss: 0.01848813\n",
            "Epoch [543000/2000000], Loss: 0.01845926\n",
            "Epoch [544000/2000000], Loss: 0.01843042\n",
            "Epoch [545000/2000000], Loss: 0.01840160\n",
            "Epoch [546000/2000000], Loss: 0.01837281\n",
            "Epoch [547000/2000000], Loss: 0.01834405\n",
            "Epoch [548000/2000000], Loss: 0.01831532\n",
            "Epoch [549000/2000000], Loss: 0.01828662\n",
            "Epoch [550000/2000000], Loss: 0.01825794\n",
            "Epoch [551000/2000000], Loss: 0.01822929\n",
            "Epoch [552000/2000000], Loss: 0.01820067\n",
            "Epoch [553000/2000000], Loss: 0.01817207\n",
            "Epoch [554000/2000000], Loss: 0.01814351\n",
            "Epoch [555000/2000000], Loss: 0.01811498\n",
            "Epoch [556000/2000000], Loss: 0.01808649\n",
            "Epoch [557000/2000000], Loss: 0.01805802\n",
            "Epoch [558000/2000000], Loss: 0.01802958\n",
            "Epoch [559000/2000000], Loss: 0.01800117\n",
            "Epoch [560000/2000000], Loss: 0.01797279\n",
            "Epoch [561000/2000000], Loss: 0.01794443\n",
            "Epoch [562000/2000000], Loss: 0.01791611\n",
            "Epoch [563000/2000000], Loss: 0.01788781\n",
            "Epoch [564000/2000000], Loss: 0.01785954\n",
            "Epoch [565000/2000000], Loss: 0.01783131\n",
            "Epoch [566000/2000000], Loss: 0.01780311\n",
            "Epoch [567000/2000000], Loss: 0.01777494\n",
            "Epoch [568000/2000000], Loss: 0.01774680\n",
            "Epoch [569000/2000000], Loss: 0.01771869\n",
            "Epoch [570000/2000000], Loss: 0.01769060\n",
            "Epoch [571000/2000000], Loss: 0.01766254\n",
            "Epoch [572000/2000000], Loss: 0.01763451\n",
            "Epoch [573000/2000000], Loss: 0.01760651\n",
            "Epoch [574000/2000000], Loss: 0.01757853\n",
            "Epoch [575000/2000000], Loss: 0.01755058\n",
            "Epoch [576000/2000000], Loss: 0.01752266\n",
            "Epoch [577000/2000000], Loss: 0.01749477\n",
            "Epoch [578000/2000000], Loss: 0.01746690\n",
            "Epoch [579000/2000000], Loss: 0.01743907\n",
            "Epoch [580000/2000000], Loss: 0.01741125\n",
            "Epoch [581000/2000000], Loss: 0.01738347\n",
            "Epoch [582000/2000000], Loss: 0.01735573\n",
            "Epoch [583000/2000000], Loss: 0.01732802\n",
            "Epoch [584000/2000000], Loss: 0.01730034\n",
            "Epoch [585000/2000000], Loss: 0.01727268\n",
            "Epoch [586000/2000000], Loss: 0.01724506\n",
            "Epoch [587000/2000000], Loss: 0.01721745\n",
            "Epoch [588000/2000000], Loss: 0.01718988\n",
            "Epoch [589000/2000000], Loss: 0.01716234\n",
            "Epoch [590000/2000000], Loss: 0.01713482\n",
            "Epoch [591000/2000000], Loss: 0.01710733\n",
            "Epoch [592000/2000000], Loss: 0.01707987\n",
            "Epoch [593000/2000000], Loss: 0.01705244\n",
            "Epoch [594000/2000000], Loss: 0.01702503\n",
            "Epoch [595000/2000000], Loss: 0.01699765\n",
            "Epoch [596000/2000000], Loss: 0.01697031\n",
            "Epoch [597000/2000000], Loss: 0.01694299\n",
            "Epoch [598000/2000000], Loss: 0.01691570\n",
            "Epoch [599000/2000000], Loss: 0.01688845\n",
            "Epoch [600000/2000000], Loss: 0.01686121\n",
            "Epoch [601000/2000000], Loss: 0.01683401\n",
            "Epoch [602000/2000000], Loss: 0.01680683\n",
            "Epoch [603000/2000000], Loss: 0.01677969\n",
            "Epoch [604000/2000000], Loss: 0.01675257\n",
            "Epoch [605000/2000000], Loss: 0.01672547\n",
            "Epoch [606000/2000000], Loss: 0.01669841\n",
            "Epoch [607000/2000000], Loss: 0.01667137\n",
            "Epoch [608000/2000000], Loss: 0.01664438\n",
            "Epoch [609000/2000000], Loss: 0.01661742\n",
            "Epoch [610000/2000000], Loss: 0.01659051\n",
            "Epoch [611000/2000000], Loss: 0.01656364\n",
            "Epoch [612000/2000000], Loss: 0.01653680\n",
            "Epoch [613000/2000000], Loss: 0.01650998\n",
            "Epoch [614000/2000000], Loss: 0.01648319\n",
            "Epoch [615000/2000000], Loss: 0.01645643\n",
            "Epoch [616000/2000000], Loss: 0.01642969\n",
            "Epoch [617000/2000000], Loss: 0.01640299\n",
            "Epoch [618000/2000000], Loss: 0.01637632\n",
            "Epoch [619000/2000000], Loss: 0.01634968\n",
            "Epoch [620000/2000000], Loss: 0.01632307\n",
            "Epoch [621000/2000000], Loss: 0.01629648\n",
            "Epoch [622000/2000000], Loss: 0.01626993\n",
            "Epoch [623000/2000000], Loss: 0.01624340\n",
            "Epoch [624000/2000000], Loss: 0.01621689\n",
            "Epoch [625000/2000000], Loss: 0.01619042\n",
            "Epoch [626000/2000000], Loss: 0.01616397\n",
            "Epoch [627000/2000000], Loss: 0.01613755\n",
            "Epoch [628000/2000000], Loss: 0.01611116\n",
            "Epoch [629000/2000000], Loss: 0.01608480\n",
            "Epoch [630000/2000000], Loss: 0.01605846\n",
            "Epoch [631000/2000000], Loss: 0.01603215\n",
            "Epoch [632000/2000000], Loss: 0.01600587\n",
            "Epoch [633000/2000000], Loss: 0.01597962\n",
            "Epoch [634000/2000000], Loss: 0.01595339\n",
            "Epoch [635000/2000000], Loss: 0.01592719\n",
            "Epoch [636000/2000000], Loss: 0.01590102\n",
            "Epoch [637000/2000000], Loss: 0.01587488\n",
            "Epoch [638000/2000000], Loss: 0.01584877\n",
            "Epoch [639000/2000000], Loss: 0.01582268\n",
            "Epoch [640000/2000000], Loss: 0.01579661\n",
            "Epoch [641000/2000000], Loss: 0.01577056\n",
            "Epoch [642000/2000000], Loss: 0.01574454\n",
            "Epoch [643000/2000000], Loss: 0.01571856\n",
            "Epoch [644000/2000000], Loss: 0.01569259\n",
            "Epoch [645000/2000000], Loss: 0.01566666\n",
            "Epoch [646000/2000000], Loss: 0.01564075\n",
            "Epoch [647000/2000000], Loss: 0.01561487\n",
            "Epoch [648000/2000000], Loss: 0.01558904\n",
            "Epoch [649000/2000000], Loss: 0.01556323\n",
            "Epoch [650000/2000000], Loss: 0.01553746\n",
            "Epoch [651000/2000000], Loss: 0.01551170\n",
            "Epoch [652000/2000000], Loss: 0.01548598\n",
            "Epoch [653000/2000000], Loss: 0.01546029\n",
            "Epoch [654000/2000000], Loss: 0.01543463\n",
            "Epoch [655000/2000000], Loss: 0.01540899\n",
            "Epoch [656000/2000000], Loss: 0.01538337\n",
            "Epoch [657000/2000000], Loss: 0.01535779\n",
            "Epoch [658000/2000000], Loss: 0.01533224\n",
            "Epoch [659000/2000000], Loss: 0.01530671\n",
            "Epoch [660000/2000000], Loss: 0.01528121\n",
            "Epoch [661000/2000000], Loss: 0.01525574\n",
            "Epoch [662000/2000000], Loss: 0.01523029\n",
            "Epoch [663000/2000000], Loss: 0.01520488\n",
            "Epoch [664000/2000000], Loss: 0.01517949\n",
            "Epoch [665000/2000000], Loss: 0.01515413\n",
            "Epoch [666000/2000000], Loss: 0.01512879\n",
            "Epoch [667000/2000000], Loss: 0.01510349\n",
            "Epoch [668000/2000000], Loss: 0.01507821\n",
            "Epoch [669000/2000000], Loss: 0.01505297\n",
            "Epoch [670000/2000000], Loss: 0.01502780\n",
            "Epoch [671000/2000000], Loss: 0.01500266\n",
            "Epoch [672000/2000000], Loss: 0.01497755\n",
            "Epoch [673000/2000000], Loss: 0.01495247\n",
            "Epoch [674000/2000000], Loss: 0.01492742\n",
            "Epoch [675000/2000000], Loss: 0.01490240\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "import pandas as pd\n",
        "\n",
        "from ucimlrepo import fetch_ucirepo\n",
        "\n",
        "# fetch dataset\n",
        "computer_hardware = fetch_ucirepo(id=29)\n",
        "\n",
        "# data (as pandas dataframes)\n",
        "feature_col = [\"MYCT\", \"CACH\"]\n",
        "not_feature_col = [\"ModelName\", \"VendorName\", \"PRP\", \"ERP\", \"MMIN\", \"CHMIN\",\"MMAX\", \"CHMAX\"]\n",
        "target_col = \"PRP\"\n",
        "X = computer_hardware.data.features.drop(columns=not_feature_col)\n",
        "for col in feature_col:\n",
        "  X[col] = X[col] /X[col].abs().max()\n",
        "X = torch.tensor((X.to_numpy())).float()\n",
        "print(X)\n",
        "y = computer_hardware.data.features[target_col]\n",
        "y = nn.functional.normalize(torch.tensor((y.to_numpy())).float(), dim=0)\n",
        "\n",
        "# Parameters\n",
        "input_size = len(X[0]) # input dimensionality\n",
        "hidden_size = int(input_size)*20\n",
        "output_size = 1\n",
        "num_epochs = 2000000 # how many iterations to run / train?\n",
        "learning_rate = 2*10**(-9) # talk about in class!\n",
        "\n",
        "# Linear regression model\n",
        "model = nn.Sequential(\n",
        "            nn.Linear(input_size, hidden_size),\n",
        "            nn.LeakyReLU(negative_slope=1.0),\n",
        "            nn.Linear(hidden_size, int(hidden_size*2)),\n",
        "            nn.LeakyReLU(negative_slope=1.0),\n",
        "            nn.Linear(hidden_size*2, output_size)\n",
        "        )\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) # says use stochastic gradient descent\n",
        "\n",
        "loss_arr = []\n",
        "epoch_arr = []\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(num_epochs): # train for num_epochs\n",
        "    # Forward pass\n",
        "    outputs = model(X) # what outputs given our current weights?\n",
        "    loss = criterion(outputs, y) # evaluate the loss\n",
        "\n",
        "    # Backward error and optimize!\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    loss_arr.append(loss.item())\n",
        "    epoch_arr.append(epoch)\n",
        "\n",
        "        # just some simple reporting\n",
        "    if (epoch+1) % 1000 == 0:\n",
        "        print ('Epoch [{}/{}], Loss: {:.8f}'.format(epoch+1, num_epochs, loss.item()))\n",
        "\n",
        "predicted_arr = []\n",
        "with torch.no_grad():\n",
        "  for (input, output) in zip(X,y):\n",
        "    predicted = model(input)\n",
        "    #print(f\"Predicted: {predicted[0]}, Actual: {output.float()}, Difference: {predicted[0]-output.float()}\")\n",
        "    predicted_arr.append(predicted[0])\n",
        "\n",
        "from sklearn.metrics import r2_score\n",
        "score = r2_score(y, predicted_arr)\n",
        "print(\"The accuracy of our model is {}%\".format(round(score, 2) *100))\n",
        "\n",
        "# Save the model checkpoint\n",
        "torch.save(model.state_dict(), 'model.ckpt')\n",
        "\n",
        "_, ax=plt.subplots()\n",
        "ax.plot(epoch_arr,loss_arr,'k')\n",
        "ax.grid()\n",
        "ax.set_title(f'Loss vs. Epoch')\n",
        "ax.set_xlabel('Epoch')\n",
        "ax.set_ylabel('Loss')\n",
        "ax.set_xticks(np.arange(0,num_epochs,num_epochs/10))\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}